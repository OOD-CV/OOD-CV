# OOD-CV dataset

## Dataset download

Our dataset can be accessed from [here](https://drive.google.com/drive/folders/1eCzhLJn9tNFDgrsHJrXzOBchpkIIC07a?usp=sharing), this google drive folder contains the data used for the challenge, and will be updated as the ICCV 2023 challenge progresses.

For accessing the full data for research purposes, please see the [OOD-CV website](https://bzhao.me/OOD-CV/).


<!-- ## Evaluation -->

<!-- Our aim is to measure model robustness w.r.t. OOD nuisance factors. Therefore, the final benchmark scoring is data and accuracy constrained. This means, that to be valid a submission must: -->
<!-- 1) Only use the training data that we provide. Using outside data is not allowed. -->
<!-- 2) The modelâ€™s accuracy on the I.I.D. test set must be lower than a pre-defined threshold (which is defined by the performance of a baseline model). -->
<!-- The final benchmark score is then measured as average performance on the held-out O.O.D. test set. -->

<!-- The I.I.D. accuracy thresholds are as follows: -->
<!-- Image Classification = 91.1 [top-1 accuracy] -->
<!-- Object Detection = 79.9 [mAP@50] -->
<!-- Pose Estimation = 68.7 [Acc@pi/6] -->
<!-- Each accuracy threshold was determined by training the baseline models five times, followed by computing the mean performance and adding three standard deviations. -->

The evaluation code used on the CodaLab server is provided in the `evaluation` folder.

## CodaLab Servers

| Tasks                | CodaLab                                            |
|----------------------|----------------------------------------------------|
| Image-Classification | TBD |
| Object-Detection     | TBD |
| 3D-Pose-Estimation   | TBD |

<!-- --- -->

<!-- This is the official repository for the [OOD-CV](https://arxiv.org/abs/2111.14341) dataset. -->



<!-- ## Q&A -->







